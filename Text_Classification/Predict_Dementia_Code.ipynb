{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # To load the saved SVM model and TF-IDF vectorizer\n",
    "import numpy as np  # For numerical operations\n",
    "from scipy.sparse import csr_matrix  # For working with sparse matrices (used in TF-IDF)\n",
    "from sklearn.svm import SVC  # For support vector classifier (if not already imported during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Non-Dementia\n"
     ]
    }
   ],
   "source": [
    "def custom_bangla_tokenizer(text):\n",
    "    return text.split() \n",
    "\n",
    "# Load the TF-IDF vectorizer with the custom tokenizer\n",
    "with open(r'C:\\Users\\mdnah\\OneDrive\\Desktop\\TextClassification\\TFIDF_Vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Load the SVM model\n",
    "with open(r'C:\\Users\\mdnah\\OneDrive\\Desktop\\TextClassification\\SVM_Model.pkl', 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "# Input sentence for prediction\n",
    "input_sentence = \"শীতের ছুটির জন্য একটা বড় প্ল্যান করছি এবার।\"\n",
    "\n",
    "# Transform the input sentence using the TF-IDF vectorizer\n",
    "input_vector = tfidf_vectorizer.transform([input_sentence])\n",
    "\n",
    "# Predict the class using the SVM model\n",
    "y_pred_rf = rf_model.predict(input_vector)\n",
    "\n",
    "# Reverse label mapping\n",
    "class_mapping = {0: \"Dementia\", 1: \"Non-Dementia\"}\n",
    "predicted_class = class_mapping[y_pred_rf[0]]\n",
    "\n",
    "# Output the prediction\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Sentence: আমার রুটি দিয়ে হাঁসের মাংস খেতে ইচ্ছা করছে\n"
     ]
    }
   ],
   "source": [
    "# Load the saved BiLSTM model\n",
    "model_save_path = r'C:\\Users\\AsifAK\\Desktop\\Code_Detect_Dementia\\SenComplete\\BiLSTM_model.h5'\n",
    "model = load_model(model_save_path)\n",
    "#print(f\"Model loaded from: {model_save_path}\")\n",
    "\n",
    "# Load the saved tokenizer\n",
    "tokenizer_save_path = r\"C:\\Users\\AsifAK\\Desktop\\Code_Detect_Dementia\\SenComplete\\sac_tokenizer.pkl\"\n",
    "with open(tokenizer_save_path, 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "#print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "# Function to complete the sentence\n",
    "def complete_sentence_with_end_token(model, tokenizer, seed_text, max_sequence_len, max_words=10):\n",
    "    completed_sentence = seed_text\n",
    "    for _ in range(max_words):\n",
    "        token_list = tokenizer.texts_to_sequences([completed_sentence])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # Stop if the word is the <end> token\n",
    "        if output_word == \"end\":\n",
    "            break\n",
    "\n",
    "        # Append the predicted word to the sentence\n",
    "        completed_sentence += \" \" + output_word\n",
    "\n",
    "        completed_sentence = completed_sentence.replace(\" end\", \"\")\n",
    "\n",
    "    return completed_sentence\n",
    "\n",
    "# Initial seed text\n",
    "seed_text = \"আমার রুটি দিয়ে হাঁসের\"  # Replace with your Bangla text সকালে আমি নাস্তা করেছিলাম  এবার করবানির ছাগলের    আমার রুটি দিয়ে হাঁসের  এই জামাটা পরতে\n",
    "\n",
    "# Define the number of words to predict\n",
    "#num_words_to_predict = 5 # Adjust based on your desired completion length\n",
    "max_sequence_len = 15\n",
    "# Complete the sentence\n",
    "completed_sentence = complete_sentence_with_end_token(model, tokenizer, seed_text, max_sequence_len)\n",
    "print(f\"Completed Sentence: {completed_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import noisereduce as nr  # Import the noise reduction library\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from tensorflow.keras.models import load_model  # type: ignore\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Paths to your model and scaler\n",
    "MODEL_PATH = r'C:\\Users\\AsifAK\\Desktop\\TextDementia\\Dementia website - Edited\\models\\lstm_model.h5'\n",
    "SCALER_PATH = r'C:\\Users\\AsifAK\\Desktop\\TextDementia\\Dementia website - Edited\\models\\scaler.pkl'\n",
    "\n",
    "# Load the model and scaler\n",
    "model = load_model(MODEL_PATH)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "# Ensure uploads folder exists\n",
    "if not os.path.exists('uploads'):\n",
    "    os.makedirs('uploads')\n",
    "\n",
    "# Function to extract features from the audio file with noise reduction\n",
    "def extract_features(audio_path):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "        # Apply noise reduction\n",
    "        y_denoised = nr.reduce_noise(y=y, sr=sr)  # Reduce noise from the audio\n",
    "\n",
    "        # Extract MFCCs (Mel Frequency Cepstral Coefficients)\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=y_denoised, sr=sr, n_mfcc=13), axis=1)\n",
    "\n",
    "        # Extract Chroma (Chroma features are related to pitch)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y_denoised, sr=sr), axis=1)\n",
    "\n",
    "        # Extract Spectral Centroid\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y_denoised, sr=sr))\n",
    "\n",
    "        # Extract Spectral Bandwidth\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y_denoised, sr=sr))\n",
    "\n",
    "        # Extract Zero-Crossing Rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=y_denoised))\n",
    "\n",
    "        # Extract Root Mean Square (RMS) energy\n",
    "        rms = np.mean(librosa.feature.rms(y=y_denoised))\n",
    "\n",
    "        # Extract Jitter (Pitch variation)\n",
    "        pitches, _ = librosa.piptrack(y=y_denoised, sr=sr)\n",
    "        pitch_values = pitches[pitches > 0]\n",
    "        jitter = np.mean(np.abs(np.diff(pitch_values)) / pitch_values[:-1]) if len(pitch_values) > 1 else 0\n",
    "\n",
    "        # Combine all the features into a single feature vector\n",
    "        feature_vector = np.hstack([mfcc, chroma, spectral_centroid, spectral_bandwidth, \n",
    "                                    zero_crossing_rate, rms, jitter])\n",
    "\n",
    "        # Reshape the feature vector into a 3D array: (1, 1, 30)\n",
    "        feature_vector_reshaped = np.expand_dims(feature_vector, axis=0)  # Shape: (1, 30)\n",
    "        feature_vector_reshaped = np.expand_dims(feature_vector_reshaped, axis=1)  # Shape: (1, 1, 30)\n",
    "\n",
    "        # Scale the features using the scaler\n",
    "        feature_vector_scaled = scaler.transform(feature_vector_reshaped.reshape(1, -1))  # Flatten for scaling\n",
    "        feature_vector_scaled = feature_vector_scaled.reshape(1, 1, -1)  # Reshape back to (1, 1, 30)\n",
    "\n",
    "        return feature_vector_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        if 'file' not in request.files:\n",
    "            return jsonify({'error': 'No file part'}), 400\n",
    "\n",
    "        file = request.files['file']\n",
    "\n",
    "        if file.filename == '':\n",
    "            return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "        if file and file.filename.endswith('.wav'):\n",
    "            file_path = os.path.join('uploads', file.filename)\n",
    "            file.save(file_path)\n",
    "\n",
    "            # Debug: Log file details\n",
    "            print(f\"Received file: {file.filename}\")\n",
    "            print(f\"File path: {file_path}\")\n",
    "\n",
    "            features = extract_features(file_path)\n",
    "            prediction = model.predict(features)\n",
    "\n",
    "            print(f\"Prediction result: {prediction}\")\n",
    "\n",
    "            prediction_label = 'Dementia' if prediction[0][0] > 0.5 else 'Non-Dementia'\n",
    "            return jsonify({'prediction': prediction_label}), 200\n",
    "        else:\n",
    "            return jsonify({'error': 'Invalid file format, only .wav allowed'}), 400\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return jsonify({'error': 'An error occurred during prediction'}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
